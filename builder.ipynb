{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "from shortid import ShortId\n",
    "from rdflib import plugin, ConjunctiveGraph, Graph, URIRef, Literal, BNode\n",
    "from rdflib.store import Store\n",
    "from rdflib.namespace import RDF, RDFS, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _l(g, d, n, k, t):\n",
    "    if isinstance(k, list):\n",
    "        p = vocab[k[0]]\n",
    "        o = d[k[1]]\n",
    "    else:\n",
    "        p = vocab[k]\n",
    "        o = d[k]\n",
    "\n",
    "    if o == 'VOID':\n",
    "        return\n",
    "    \n",
    "    g.add((n, p, Literal(o, datatype=t)))\n",
    "    \n",
    "def _b(g, n, k, b):\n",
    "    g.add((n, vocab[k], b))\n",
    "    \n",
    "def _r(g, d, n, k):\n",
    "    if isinstance(k, list):\n",
    "        p = vocab[k[0]]\n",
    "        if d is None:\n",
    "            o = vocab['NULL']\n",
    "        elif d == 'VOID':\n",
    "            o = vocab['VOID']\n",
    "        elif d == 'none':\n",
    "            o = vocab['none']\n",
    "        elif d == 'partially':\n",
    "            o = vocab['partially']\n",
    "        else: \n",
    "            o = d[k[1]]\n",
    "    else:\n",
    "        p = vocab[k]\n",
    "        o = d[k]\n",
    "        \n",
    "    if o == 'VOID':\n",
    "        return\n",
    "    \n",
    "    if o is None:\n",
    "        g.add((n, p, URIRef(vocab[o])))\n",
    "        return\n",
    "    \n",
    "    if o.find('http') > -1 or o.find('www') > -1 or o.find('@') > -1:\n",
    "        g.add((n, p, URIRef(o)))\n",
    "        return\n",
    "        \n",
    "    g.add((n, p, URIRef(vocab[o])))\n",
    "    \n",
    "def _t(g, d, n, k):\n",
    "    if k in d:\n",
    "        g.add((n, RDF.type, vocab[d[k]]))\n",
    "    else:\n",
    "        g.add((n, RDF.type, vocab[k]))\n",
    "    \n",
    "def _c(g, d, n1, n2, k):\n",
    "    _b(g, n1, k, n2)\n",
    "    _t(g, d, n2, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n2, e)\n",
    "        \n",
    "def _li(g, n, e):\n",
    "    g.add((n, vocab['li'], URIRef(vocab[e])))\n",
    "    \n",
    "def process(d):\n",
    "    gid = URIRef('{}G{}'.format(ns, sid.generate()))\n",
    "    g = Graph(store, gid)\n",
    "    process_survey(g, d['survey'])\n",
    "    process_infrastructure(g, d['infrastructure'])\n",
    "    \n",
    "def process_survey(g, d):\n",
    "    n = BNode()\n",
    "    _l(g, d, g.identifier, 'date', XSD.date)\n",
    "    _l(g, d, g.identifier, 'version', XSD.string)\n",
    "    process_creator(g, d['creator'], g.identifier)\n",
    "    \n",
    "def process_creator(g, d, n):\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'creator', n1)\n",
    "    _l(g, d, n1, 'name', XSD.string)\n",
    "    _r(g, d, n1, 'email')\n",
    "    \n",
    "def process_infrastructure(g, d):\n",
    "    n = BNode()\n",
    "    _t(g, d, n, 'ResearchInfrastructure')\n",
    "    _l(g, d, n, 'acronym', XSD.string)\n",
    "    _l(g, d, n, ['label', 'name'], XSD.string)\n",
    "    _r(g, d, n, ['url', 'recognized authority IRI'])\n",
    "    process_infrastructure_datamanagementplans(g, d['data management plans'], n)\n",
    "    for repository in d['repositories']:\n",
    "        process_repository(g, repository, n)\n",
    "    \n",
    "def process_infrastructure_datamanagementplans(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'hasDataManagementPlans')):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'hasDataManagementPlans', n1)\n",
    "    _l(g, d, n1, ['usesSpecificDataManagementPlanTools', 'specific DMP tools used'], XSD.bool)\n",
    "    _l(g, d, n1, ['appliedDataPublishingSteps', 'data publishing steps applied'], XSD.string)\n",
    "        \n",
    "def process_repository(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'hasRepository')):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'hasRepository', n1)\n",
    "    _t(g, d, n1, 'Repository')\n",
    "    _l(g, d, n1, ['label', 'name'], XSD.string)  \n",
    "    _r(g, d, n1, ['url', 'IRI'])\n",
    "    _t(g, d, n1, 'kind')\n",
    "    _r(g, d, n1, ['usesSystem', 'software'])\n",
    "    process_repository_identifier(g, d['identifier'], n1)\n",
    "    _l(g, d, n1, ['hasCertificationMethod', 'certification method'], XSD.string)\n",
    "    _l(g, d, n1, ['hasPolicy', 'policy'], XSD.string)\n",
    "    _r(g, d, n1, ['inRegistry', 'registry'])\n",
    "    _l(g, d, n1, ['hasPersistencyGuaranty', 'persistency-guaranty'], XSD.string)\n",
    "    process_repository_access(g, d['access mechanisms'], n1)\n",
    "    process_repository_data(g, d['data'], n1)\n",
    "    process_repository_metadata(g, d['metadata'], n1)\n",
    "    process_repository_vocabularies(g, d['vocabularies'], n1)\n",
    "    process_repository_dataprocessing(g, d['data processing'], n1)\n",
    "    process_repository_fairness(g, d['fairness'], n1)\n",
    "    process_repository_testfairness(g, d['test fairness'], n1)\n",
    "        \n",
    "        \n",
    "def process_repository_identifier(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'usesIdentifier')):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'usesIdentifier', n1)\n",
    "    _t(g, d, n1, 'Identifier')\n",
    "    _t(g, d, n1, 'kind')\n",
    "    _r(g, d, n1, ['usesSystem', 'system'])\n",
    "    _l(g, d, n1, ['isAssigned', 'assigned'], XSD.string)\n",
    "    _r(g, d, n1, ['usesProvider', 'provider'])\n",
    "    \n",
    "    \n",
    "def process_repository_access(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'hasAccessMechanisms')):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'hasAccessMechanisms', n1)\n",
    "    _t(g, d, n1, 'AccessMechanism')\n",
    "    _l(g, d, n1, ['hasAuthenticationMethod', 'authentication method'], XSD.string)\n",
    "    _r(g, d, n1, ['hasAccessProtocolUrl', 'access protocol URL'])\n",
    "    _l(g, d, n1, ['protocolIsOpen', 'protocol open'], XSD.bool)\n",
    "    _l(g, d, n1, ['protocolIsRoyaltyFree', 'protocol royalty free'], XSD.bool)\n",
    "    _l(g, d, n1, ['maintainsOwnUserDatabase', 'own user database maintained'], XSD.bool)\n",
    "    _l(g, d, n1, ['usesORCIDinAAI', 'ORCID used in AAI'], XSD.bool)\n",
    "    _r(g, d, n1, ['supportsMajorAccessTechnology', 'major access technology supported'])\n",
    "    _r(g, d, n1, ['usesAuthorisationTechnique', 'authorisation technique'])\n",
    "    process_authorisation_needed_for(g, d['authorisation needed for'], n1)\n",
    "    _l(g, d, n1, ['authorisationRequired', 'authorization for accessing content needed'], XSD.bool)\n",
    "    _r(g, d, n1, ['hasAccessConcentProcessDescriptionUri', 'access content process description IRI'])\n",
    "    _r(g, d, n1, ['usesDataLicenses', 'data licenses in use'])\n",
    "    _r(g, d, n1, ['dataLicenseIri', 'data license IRI'])\n",
    "    _l(g, d, n1, ['openAccessMetadata', 'metadata openly available'], XSD.bool)\n",
    "    \n",
    "    \n",
    "def process_authorisation_needed_for(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'usesAuthorisationFor')):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'usesAuthorisationFor', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "    \n",
    "def process_repository_data(g, d, n):\n",
    "    if (handle_special_cases(g, d, n, 'hasData')):\n",
    "        return\n",
    "    for e1 in d:\n",
    "        n1 = BNode()\n",
    "        _b(g, n, 'hasData', n1)\n",
    "        _t(g, e1, n1, 'Data')\n",
    "        _t(g, e1, n1, 'type name')\n",
    "        _l(g, e1, n, ['dataSchemaIsRegistered', 'registered data schema'], XSD.bool)\n",
    "        _l(g, e1, n, ['searchOnData', 'search on data'], XSD.bool)\n",
    "        _r(g, e1, n, ['hasSearchEngineUrl', 'search engine URL'])\n",
    "        for e2 in e1['preferred formats']:\n",
    "            n2 = BNode()\n",
    "            _b(g, n1, 'hasPreferredFormat', n2)\n",
    "            _t(g, e2, n2, 'PreferredFormat')\n",
    "            _r(g, e2, n2, ['hasFormatName', 'format name'])\n",
    "            e3 = e2['metadata types in data headers']\n",
    "            if e3 is None:\n",
    "                _r(g, e3, n1, ['hasDataHeaderMetadataTypes', 'NULL'])\n",
    "            elif e3 == 'none':\n",
    "                _r(g, e3, n1, ['hasDataHeaderMetadataTypes', 'None'])\n",
    "            else:\n",
    "                n3 = BNode()\n",
    "                _b(g, n2, 'hasDataHeaderMetadataTypes', n3)\n",
    "                _t(g, e3, n3, 'Bag')\n",
    "                for e in e3:\n",
    "                    _li(g, n3, e)\n",
    "                \n",
    "\n",
    "def process_repository_metadata(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'hasMetadata'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'hasMetadata', n1)\n",
    "    process_repository_metadata_schema(g, d['schema'], n1)\n",
    "    _l(g, d, n1, ['categoriesAreDefinedInRegistries', 'categories defined in registries'], XSD.bool)\n",
    "    _l(g, d, n1, ['persistentIdentifiersAreIncluded', 'PIDs included'], XSD.bool)\n",
    "    _r(g, d, n1, ['hasPrimaryStorageFormat', 'primary storage format'])\n",
    "    _r(g, d, n1, ['hasMetadataLongevityPlan', 'metadata longevity plan URL'])\n",
    "    _r(g, d, n1, ['hasFormat', 'format IRI'])\n",
    "    n2 = BNode()\n",
    "    _b(g, n1, 'supportedExportFormats', n2)\n",
    "    _t(g, d, n2, 'Bag')\n",
    "    for e in d['export format supported']:\n",
    "        _li(g, n2, e)\n",
    "    n3 = BNode()\n",
    "    _b(g, n1, 'hasHarvestingMethod', n3)\n",
    "    _t(g, d, n3, 'Bag')\n",
    "    for e in d['exchange/harvesting method']:\n",
    "        _li(g, n3, e)\n",
    "    _r(g, d, n1, ['hasLocalSearchEngine', 'local search engine URL'])\n",
    "    _r(g, d, n1, ['supportsExternalSearchEngineType', 'external search engine type supported'])\n",
    "    _l(g, d, n1, ['includesAccessPolicyStatements', 'access policy statements included'], XSD.bool)\n",
    "    _l(g, d, n1, ['isMachineActionable', 'machine actionable'], XSD.bool)\n",
    "\n",
    "\n",
    "def process_repository_metadata_schema(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'hasSchema'):\n",
    "        return\n",
    "    for e1 in d:\n",
    "        n1 = BNode()\n",
    "        _b(g, n, 'hasSchema', n1)\n",
    "        _r(g, e1, n1, ['hasIri', 'IRI'])\n",
    "        _r(g, e1, n1, ['hasName', 'name'])\n",
    "        e2 = e1['provenance fields included']\n",
    "        if e2 is None:\n",
    "            _r(g, e2, n1, ['includesProvenanceFields', 'None'])\n",
    "        elif e2 == 'partially':\n",
    "            _r(g, e2, n1, ['includesProvenanceFields', 'partially'])\n",
    "        else:\n",
    "            n2 = BNode()\n",
    "            _b(g, n1, 'includesProvenanceFields', n2)\n",
    "            _t(g, e1, n2, 'Bag')\n",
    "            for e2 in e2:\n",
    "                _li(g, n2, e2)\n",
    "    \n",
    "    \n",
    "def process_repository_vocabularies(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'hasVocabularies'):\n",
    "        return\n",
    "    for e1 in d:\n",
    "        n1 = BNode()\n",
    "        _b(g, n, 'hasVocabularies', n1)\n",
    "        _r(g, e1, n1, ['hasIri', 'IRI'])\n",
    "        _t(g, e1, n1, 'type')\n",
    "        _r(g, e1, n1, ['hasTopic', 'topic'])\n",
    "        _l(g, e1, n1, ['hasName', 'name'], XSD.string)\n",
    "        _r(g, e1, n1, ['hasSpecificationLanguage', 'specification language URL'])\n",
    "\n",
    "def process_repository_dataprocessing(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'hasDataProcessing'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'hasDataProcessing', n1)\n",
    "    process_repository_dataprocessing_special(g, d['special data processing steps applied'], n1)\n",
    "    process_repository_dataprocessing_workflow(g, d['workflow frameworks applied'], n1)\n",
    "    process_repository_dataprocessing_distributed(g, d['distributed workflows tools used'], n1)\n",
    "    process_repository_dataprocessing_other(g, d['other analysis services offered'], n1)\n",
    "    process_repository_dataprocessing_data(g, d['data products offered'], n1)\n",
    "\n",
    "\n",
    "def process_repository_dataprocessing_special(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'specialDataProcessingStepsApplied'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'specialDataProcessingStepsApplied', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "    \n",
    "\n",
    "def process_repository_dataprocessing_workflow(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'workflowFrameworksApplied'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'workflowFrameworksApplied', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "\n",
    "def process_repository_dataprocessing_distributed(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'distributedWorkflowsToolsUsed'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'distributedWorkflowsToolsUsed', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "\n",
    "def process_repository_dataprocessing_other(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'otherAnalysisServicesOffered'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'otherAnalysisServicesOffered', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "\n",
    "def process_repository_dataprocessing_data(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'dataProductsOffered'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'dataProductsOffered', n1)\n",
    "    _t(g, d, n1, 'Bag')\n",
    "    for e in d:\n",
    "        _li(g, n1, e)\n",
    "    \n",
    "def process_repository_fairness(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'fairness'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'fairness', n1)\n",
    "    process_repository_faireness_findability(g, d['data findability'], n1)\n",
    "    process_repository_faireness_accessibility(g, d['data accessibility'], n1)\n",
    "    process_repository_faireness_interoperability(g, d['data interoperability'], n1)\n",
    "    process_repository_faireness_reusability(g, d['data re-usability'], n1)\n",
    "\n",
    "def process_repository_faireness_findability(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'dataFindability'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'dataFindability', n1)\n",
    "    _l(g, d, n1, ['dataIsFindable', 'data findable'], XSD.bool)\n",
    "    process_repository_faireness_findability_gaps(g, d['gaps'], n1)\n",
    "\n",
    "def process_repository_faireness_accessibility(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'dataAccessibility'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'dataAccessibility', n1)\n",
    "    _l(g, d, n1, ['dataIsAccessible', 'data accessible'], XSD.bool)\n",
    "    process_repository_faireness_accessibility_gaps(g, d['gaps'], n1)\n",
    "\n",
    "def process_repository_faireness_interoperability(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'dataInteroperability'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'dataInteroperability', n1)\n",
    "    _l(g, d, n1, ['dataIsInteroperable', 'data interoperable'], XSD.bool)\n",
    "    process_repository_faireness_interoperability_gaps(g, d['gaps'], n1)\n",
    "\n",
    "def process_repository_faireness_reusability(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'dataReusability'):\n",
    "        return\n",
    "    n1 = BNode()\n",
    "    _b(g, n, 'dataReusability', n1)\n",
    "    _l(g, d, n1, ['dataIsReusable', 'data reusable'], XSD.bool)\n",
    "    process_repository_faireness_reusability_gaps(g, d['gaps'], n1)\n",
    "    \n",
    "def process_repository_faireness_findability_gaps(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'gaps'):\n",
    "        return\n",
    "    _c(g, d, n, BNode(), 'gaps')\n",
    "    \n",
    "def process_repository_faireness_accessibility_gaps(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'gaps'):\n",
    "        return\n",
    "    _c(g, d, n, BNode(), 'gaps')\n",
    "\n",
    "def process_repository_faireness_interoperability_gaps(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'gaps'):\n",
    "        return\n",
    "    _c(g, d, n, BNode(), 'gaps')\n",
    "    \n",
    "def process_repository_faireness_reusability_gaps(g, d, n):\n",
    "    if handle_special_cases(g, d, n, 'gaps'):\n",
    "        return\n",
    "    _c(g, d, n, BNode(), 'gaps')\n",
    "    \n",
    "def process_repository_testfairness(g, d, n):\n",
    "    _r(g, d, n, ['hasDataset', 'URL/IRI of dataset'])\n",
    "    _r(g, d, n, ['hasDiscoveryPortal', 'URL of discovery portal'])\n",
    "    _r(g, d, n, ['hasMachineReadableDatasetMetadata', 'IRI of machine readable metadata of dataset'])\n",
    "    _r(g, d, n, ['hasLinksetDescribingResource', 'URL to linkset describing resource'])\n",
    "    _l(g, d, n, ['hasSearchQuery', 'Search query'], XSD.string)\n",
    "    _r(g, d, n, ['hasCitationalProvenanceVocabulary', 'IRI for the vocabulary used to describe citational provenance'])\n",
    "    _r(g, d, n, ['hasContextualProvenanceVocabulary', 'IRI for the vocabulary used to describe contextual provenance'])\n",
    "    _r(g, d, n, ['hasComplianceCertification', 'IRI of compliance certification'])\n",
    "\n",
    "def handle_special_cases(g, d, n, k):\n",
    "    if d is None:\n",
    "        _r(g, d, n, [k, 'NULL'])\n",
    "        return True\n",
    "    if d is 'NULL':\n",
    "        _r(g, d, n, [k, 'NULL'])\n",
    "        return True\n",
    "    if d == 'VOID':\n",
    "        _r(g, d, n, [k, 'VOID'])\n",
    "        return True\n",
    "    if d == 'none':\n",
    "        _r(g, d, n, [k, 'none'])\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 'http://envri.eu/ns/'\n",
    "sid = ShortId()\n",
    "store = plugin.get('IOMemory', Store)()\n",
    "\n",
    "vocab = dict()\n",
    "vocab[None] = URIRef('http://envri.eu/ns/NULL')\n",
    "vocab['partially'] = URIRef('http://envri.eu/ns/Partially')\n",
    "vocab['none'] = URIRef('http://envri.eu/ns/None')\n",
    "vocab['NULL'] = URIRef('http://envri.eu/ns/NULL')\n",
    "vocab['VOID'] = URIRef('http://envri.eu/ns/VOID')\n",
    "\n",
    "with open('vocab.yaml', 'r') as f:\n",
    "    for key, value in yaml.safe_load(f).items():\n",
    "        vocab[key] = URIRef(value)\n",
    "    \n",
    "for file in glob.glob('descriptions/*.yaml'):\n",
    "    with open(file, 'r') as f:\n",
    "        for document in yaml.load_all(f, Loader=yaml.FullLoader):\n",
    "            process(document)\n",
    "    \n",
    "g = ConjunctiveGraph(store)\n",
    "g.bind('envri', ns)\n",
    "g.bind('dcterms', 'http://purl.org/dc/terms/')\n",
    "g.bind('foaf', 'http://xmlns.com/foaf/0.1/')\n",
    "g.serialize(destination='data.trig', format='trig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from rdflib.plugins.sparql.results.csvresults import CSVResultSerializer\n",
    "\n",
    "def query(q):\n",
    "    serializer = CSVResultSerializer(g.query(q))\n",
    "    output = io.BytesIO()\n",
    "    serializer.serialize(output)\n",
    "    display(pd.read_csv(io.StringIO(output.getvalue().decode('utf-8'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"\"\"\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "PREFIX envri: <http://envri.eu/ns/>\n",
    "PREFIX rm: <http://www.oil-e.net/ontology/envri-rm.owl#>\n",
    "\n",
    "SELECT ?date ?ri_acronym ?ri_url ?rep_label WHERE {\n",
    "    ?g dcterms:date ?date .\n",
    "    GRAPH ?g { \n",
    "        ?ri a rm:ResearchInfrastructure .\n",
    "        ?ri envri:acronym ?ri_acronym . \n",
    "        ?ri envri:url ?ri_url .\n",
    "        ?ri envri:hasRepository ?rep .\n",
    "        ?rep a envri:Repository .\n",
    "        ?rep rdfs:label ?rep_label .\n",
    "    }\n",
    "    FILTER (?date > \"2019-03-15\"^^xsd:date)\n",
    "}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
